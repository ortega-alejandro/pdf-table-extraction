{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter, PDFPageAggregator\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed, PDFPage\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTLine, LTFigure, LTImage, LTRect, LTTextLine\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from owlready2 import *\n",
    "from nltk.stem.porter import *\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "from nltk.stem.porter import *\n",
    "import operator\n",
    "from PyPDF2 import PdfFileReader\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################ INITIALIZE VARIABLES ##############################################################\n",
    "\n",
    "base_path = 'desktop/ml/'\n",
    "path = '3AC1DACD68A35562618B2A9D7B92DE841964B.pdf'\n",
    "my_pdf = os.path.join(base_path+\"/\"+path)\n",
    "\n",
    "#keyword frequency threshold value, for the entire document\n",
    "threshold = 0.03\n",
    "#keyword frequency threshold value, for each page\n",
    "page_threshold = 0.005\n",
    "#number of levels of the ontology to display to the user\n",
    "number_of_levels = 3\n",
    "class_names = ['DRE Technologies', 'DRE Policies', 'DRE Economics', 'DRE Impacts']\n",
    "ontology_path = \"file:///users/jadewu/downloads/root-ontology-v7.owl\"\n",
    "print_ontology = False\n",
    "print_stemmed_ontology = False\n",
    "display_frequency_matrix = False\n",
    "print_matched_words = True\n",
    "print_pmatches_by_page = True\n",
    "print_pmatches_by_keyword = True\n",
    "print_all_chains = False\n",
    "print_simplified_chains = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MATCHES BY PAGE: \n",
      "1 ['Health', 'Affordability']\n",
      "2 ['Education', 'Wind']\n",
      "3 ['Private', 'Wind']\n",
      "4 []\n",
      "5 ['Tools', 'Market', 'Cost']\n",
      "6 ['Wind', 'Cost']\n",
      "7 ['Investment', 'Cost']\n",
      "8 ['Subsidization', 'Affordability', 'Cost']\n",
      "9 ['Benefits', 'Education', 'Market', 'Cost']\n",
      "10 ['Utility', 'Innovations', 'Benefits', 'Education', 'Investment', 'Market', 'Cost', 'Customer', 'Financing']\n",
      "11 ['Wind', 'Cost']\n",
      "12 ['Benefits', 'Wind', 'Market', 'Financing']\n",
      "\n",
      "MATCHED WORDS, FOR ENTIRE DOCUMENT: \n",
      "Total number of matched words: 18\n",
      "Cost appears 60 times with frequency 0.69029\n",
      "Market appears 22 times with frequency 0.25311\n",
      "Education appears 16 times with frequency 0.18408\n",
      "Wind appears 16 times with frequency 0.18408\n",
      "Benefits appears 15 times with frequency 0.17257\n",
      "Investment appears 14 times with frequency 0.16107\n",
      "Affordability appears 10 times with frequency 0.11505\n",
      "Financing appears 10 times with frequency 0.11505\n",
      "Innovations appears 8 times with frequency 0.09204\n",
      "Subsidization appears 8 times with frequency 0.09204\n",
      "Utility appears 7 times with frequency 0.08053\n",
      "Private appears 7 times with frequency 0.08053\n",
      "Customer appears 7 times with frequency 0.08053\n",
      "Heating appears 4 times with frequency 0.04602\n",
      "Environmental impacts appears 4 times with frequency 0.04602\n",
      "Solar Thermal appears 3 times with frequency 0.03451\n",
      "Tools appears 3 times with frequency 0.03451\n",
      "Health appears 3 times with frequency 0.03451\n",
      "\n",
      "MATCHES BY KEYWORD: \n",
      "Cost [9, 10, 5, 7, 8, 6, 2, 11, 1, 3]\n",
      "Market [9, 12, 4, 5, 6, 10, 1, 2, 7, 8, 11]\n",
      "Education [10, 2, 9, 1, 3, 5, 7]\n",
      "Wind [2, 3, 6, 4, 11, 12, 8]\n",
      "Benefits [10, 9, 2, 12, 1, 4, 5, 7]\n",
      "Investment [7, 10, 3, 9, 1, 5, 6, 8]\n",
      "Affordability [8, 1, 2, 9, 10, 11]\n",
      "Financing [10, 12, 3, 8, 11]\n",
      "Innovations []\n",
      "Subsidization [8, 4, 5, 7]\n",
      "Utility [10, 1, 6, 7, 8, 12]\n",
      "Private [3, 6, 1, 10]\n",
      "Customer [10, 6, 5, 9]\n",
      "Heating [2, 3, 4]\n",
      "Environmental impacts [2, 1, 10]\n",
      "Solar Thermal [6, 3]\n",
      "Tools [5, 7]\n",
      "Health [1, 7]\n",
      "\n",
      "SIMPLIFIED CHAINS: \n",
      "Total number of chains: 15\n",
      "['DRE Policies', 'Incentivization', 'Subsidization']\n",
      "['DRE Economics', 'Market', 'Customer']\n",
      "['DRE Economics', 'Centralized Grid Technology', 'Cost']\n",
      "['DRE Economics', 'Investment', 'Financing']\n",
      "['DRE Technologies', 'Stand-alone Systems', 'Solar Thermal']\n",
      "['DRE Technologies', 'Asset Ownership Models', 'Devices and Home Systems']\n",
      "['DRE Technologies', 'Asset Ownership Models', 'Mini-Grid/Mini-Utility']\n",
      "['DRE Technologies', 'Asset Ownership Models', 'Mini-Grid/Mini-Utility']\n",
      "['DRE Technologies', 'Productive Use of Electrcity Appliances', 'Heating']\n",
      "['DRE Technologies', 'Mini-Grid Technology', 'Wind']\n",
      "['DRE Impacts', 'Socio-economic impacts', 'Affordability']\n",
      "['DRE Impacts', 'Social Impacts', 'Health']\n",
      "['DRE Impacts', 'Social Impacts', 'Education']\n",
      "['DRE Impacts', 'Environmental impacts']\n",
      "['DRE Impacts', 'Impact Evaluation']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###################################### LOAD/PROCESS THE ONTOLOGY ##################################################\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "onto = get_ontology(ontology_path).load()\n",
    "\n",
    "#store all terms in the ontology into \"arr\"\n",
    "arr = []\n",
    "for i in class_names:\n",
    "    for j in onto.search(label = i)[0].instances():\n",
    "        arr.append(j.label)\n",
    "for m in onto.classes():\n",
    "    arr.append(m.label)\n",
    "    \n",
    "if print_ontology:\n",
    "    print(\"the entire ontology: \")\n",
    "    print(arr)\n",
    "\n",
    "#stem the ontology {key, value} = {stemmed word, original word}, store into \"arr_stemmed_ontology\"\n",
    "arr_stemmed_ontology = []\n",
    "for i in range(len(arr)): \n",
    "    real_label = ''       \n",
    "    #CASE #1: ['preferred label', 'label 2', 'label 3', ...] (or, there are multiple labels for a term in ontology)\n",
    "    if (len(arr[i]) > 1):  \n",
    "        for j in range(len(arr[i])):\n",
    "            is_preflabel = len(onto.search(prefLabel = arr[i][j])) \n",
    "            if (is_preflabel == 0):      \n",
    "                continue\n",
    "            real_label = arr[i][j]     \n",
    "        if (real_label == ''):\n",
    "            raise Exception(arr[i][j] + \" has no preferred label\")     \n",
    "        for x in range(len(arr[i])):\n",
    "            arr_stemmed2 = ''\n",
    "            word = arr[i][x].split()\n",
    "            for k in range(len(word)):\n",
    "                arr_stemmed2 = arr_stemmed2 + stemmer.stem(word[k].lower()) + \" \"   \n",
    "            arr_stemmed_ontology.append((arr_stemmed2.strip(), real_label)) \n",
    "    #CASE #2: ['only label']\n",
    "    else:               \n",
    "        real_label = arr[i][0]\n",
    "        arr_stemmed2 = ''\n",
    "        word = real_label.split()\n",
    "        for k in range(len(word)):\n",
    "            arr_stemmed2 = arr_stemmed2 + stemmer.stem(word[k].lower()) + \" \"   \n",
    "        arr_stemmed_ontology.append((arr_stemmed2.strip(), \" \".join(word))) \n",
    "       \n",
    "    \n",
    "if print_stemmed_ontology:\n",
    "    print(\"\\nstemmed instances:\")\n",
    "    print(arr_stemmed_ontology)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################### FINDING MATCHES BETWEEN PDF AND ONTOLOGY #####################################\n",
    "\n",
    "\n",
    "'''CREATE A FREQUENCY MATRIX WHERE:\n",
    "    ROWS = PAGE NUMBER\n",
    "    COLUMNS = ONTOLOGY\n",
    "    CELLS = # OF TIMES A KEYWORD IN ONTOLOGY APPEARS ON PAGE __'''\n",
    "\n",
    "\n",
    "#function to process pdf text, for each page\n",
    "def process_pdf(page_text):\n",
    "    page_text = re.sub(\"\\.\", \"\", page_text)\n",
    "    page_text = re.sub(\"[0-9]+\", \"\", page_text)\n",
    "    page_text = re.sub(\"-\\n\", \"\", page_text)\n",
    "    page_text = page_text.lower()\n",
    "    page_text = re.sub(\"(\\W+)\", \" \", page_text)\n",
    "    page_text = re.sub(\"ï¬\", \"fi\", page_text)\n",
    "    pdf_arr = page_text.split()\n",
    "    pdf_arr_stemmed = []\n",
    "    for i in range(len(pdf_arr)):\n",
    "        pdf_arr_stemmed.append(stemmer.stem(pdf_arr[i]))\n",
    "    page_text = \" \".join(pdf_arr_stemmed)\n",
    "    return page_text\n",
    "\n",
    "\n",
    "#initialize frequency matrix\n",
    "frequency_matrix = []\n",
    "\n",
    "fp = open(my_pdf, \"rb\")\n",
    "parser = PDFParser(fp)\n",
    "document = PDFDocument(parser)\n",
    "if not document.is_extractable:\n",
    "    raise PDFTextExtractionNotAllowed\n",
    "rsrcmgr = PDFResourceManager()\n",
    "laparams = LAParams()\n",
    "device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "#initialize page counter\n",
    "page_number = 0\n",
    "#variable to store processed text for the whole pdf document\n",
    "doc_text = \"\"\n",
    "for page in PDFPage.create_pages(document):\n",
    "    #variable to store processed text for each page\n",
    "    page_text = \"\"\n",
    "    interpreter.process_page(page)\n",
    "    layout = device.get_result()\n",
    "    for lt_obj in layout:\n",
    "        if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n",
    "            page_text += lt_obj.get_text()\n",
    "    page_text = process_pdf(page_text)\n",
    "    doc_text += page_text\n",
    "    frequency_matrix.append([]) \n",
    "    for i in range(len(arr_stemmed_ontology)):\n",
    "        key = arr_stemmed_ontology[i][0] #[0] = stemmed keyword\n",
    "        value = arr_stemmed_ontology[i][1] #[1] = original keyword\n",
    "        regex = key\n",
    "        freq = re.findall(regex, page_text)\n",
    "        frequency_matrix[page_number].append(len(freq))\n",
    "    page_number += 1\n",
    "\n",
    "#converting frequency matrix into dataframe, renaming column headers\n",
    "column_headers = []\n",
    "for tup in arr_stemmed_ontology:\n",
    "    column_headers.append(tup[1])\n",
    "frequency_matrix_df = pd.DataFrame(frequency_matrix, columns = column_headers)\n",
    "\n",
    "#display frequency matrix\n",
    "if display_frequency_matrix:\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        display(frequency_matrix_df)\n",
    "\n",
    "        \n",
    "'''DISPLAY MATCHES, ON A PAGE BY PAGE BASIS:\n",
    "    FIRST DICTIONARY: pmatches_by_page: {key, value} = {page #, frequent word appeareances according to page_threshold}\n",
    "    SECOND DICTIONARY: pmatches_by_keyword: {key, value} = {matched word, pages that word appears on, in order of importance}'''\n",
    "\n",
    "\n",
    "pdf2 = PdfFileReader(open(my_pdf, 'rb'))\n",
    "num_of_pages = pdf2.getNumPages()\n",
    "\n",
    "'''FIRST DICTIONARY'''\n",
    "pmatches_by_page = {}\n",
    "for p in range(num_of_pages):\n",
    "    page = pdf2.getPage(p)\n",
    "    num_of_words_page = (len(page.extractText()))\n",
    "    pmatches_by_page[p] = []\n",
    "    if (num_of_words_page == 0):\n",
    "        continue\n",
    "    for x in range(len(frequency_matrix[p])):     \n",
    "        if (frequency_matrix[p][x]/num_of_words_page > page_threshold):\n",
    "                pmatches_by_page[p].append(arr_stemmed_ontology[x][1])\n",
    "\n",
    "if print_pmatches_by_page:   \n",
    "    print(\"\\nMATCHES BY PAGE: \")\n",
    "    for key in pmatches_by_page:\n",
    "        print(str(key+1), pmatches_by_page[key])\n",
    "\n",
    "\n",
    "'''FIND MATCHES BETWEEN ONTOLOGY AND THE ENTIRE DOCUMENT WHERE:\n",
    "    matches_count (dictionary) = number of times each ontology word appears in the entire document\n",
    "    matches_freq (dictionary) = number of times each ontology word appears in the entire document/total number of words'''\n",
    "\n",
    "matches_count = {}\n",
    "matches_freq = {}\n",
    "\n",
    "#num_of_words = total number of relevant words in the document\n",
    "num_of_words = len(doc_text.split())\n",
    "for i in range(len(arr_stemmed_ontology)):\n",
    "    key = arr_stemmed_ontology[i][0]\n",
    "    value = arr_stemmed_ontology[i][1]\n",
    "    regex = key\n",
    "    #print(regex)\n",
    "    freq = re.findall(regex, doc_text)\n",
    "    if (len(freq)/num_of_words)*100 < threshold:\n",
    "        continue \n",
    "    if value not in matches_count.keys():\n",
    "        matches_count[value] = len(freq)\n",
    "        matches_freq[value] = round((len(freq)/num_of_words)*100, 5) \n",
    "    else:\n",
    "        matches_count[value] += len(freq)\n",
    "        matches_freq[value] += round((len(freq)/num_of_words)*100, 5)\n",
    "\n",
    "    \n",
    "#Sort matches\n",
    "matches_count_sorted = sorted(matches_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "matches_freq_sorted = sorted(matches_freq.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "\n",
    "#initialize array of matches, sorted\n",
    "matches_sorted = [] \n",
    "for i in range(len(matches_freq_sorted)):\n",
    "    matches_sorted.append(matches_freq_sorted[i][0])\n",
    "\n",
    "\n",
    "#print matched words with count and frequency\n",
    "\n",
    "if print_matched_words:\n",
    "    print(\"\\nMATCHED WORDS, FOR ENTIRE DOCUMENT: \")\n",
    "    print(\"Total number of matched words: \" + str(len(matches_sorted)))\n",
    "    for word in matches_sorted:\n",
    "        print (word + \" appears \"+ str(matches_count[word]) + \" times with frequency \" + str(matches_freq[word]))\n",
    "\n",
    "\n",
    "'''SECOND DICTIONARY'''\n",
    "pmatches_by_keyword = {}\n",
    "for word in matches_sorted:  \n",
    "    if word not in pmatches_by_keyword.keys():\n",
    "        pmatches_by_keyword[word] = []\n",
    "    for position, header in enumerate(frequency_matrix_df.columns.values.tolist()):\n",
    "        if (header == word):\n",
    "            column_num = position\n",
    "    page_list = frequency_matrix_df.iloc[:,column_num].tolist()\n",
    "    #print(word)\n",
    "    #print(page_list)\n",
    "    while(max(page_list) != 0):\n",
    "        #print(\"while loop\")\n",
    "        index = page_list.index(max(page_list))\n",
    "        pmatches_by_keyword[word].append(index+1)\n",
    "        page_list[index] = 0\n",
    "      \n",
    "    \n",
    "    \n",
    "if print_pmatches_by_keyword:   \n",
    "    print(\"\\nMATCHES BY KEYWORD: \")\n",
    "    for key in pmatches_by_keyword:\n",
    "        print(key, pmatches_by_keyword[key])\n",
    "\n",
    "\n",
    "###################################### FINDING PARENT CHAINS OF EACH KEYWORD ####################################\n",
    "\n",
    "\n",
    "#recursive function to find the parent of a word\n",
    "\n",
    "#word = word match found; TYPE = STRING \n",
    "def findParent(word, chain):        \n",
    "    parent = onto.search(label = word)\n",
    "    parent = parent[0].is_a[0]\n",
    "    parent = parent.label\n",
    "    if len(parent) == 0:\n",
    "        return\n",
    "    chain.insert(0, parent[0])\n",
    "    findParent(parent[0], chain)\n",
    "\n",
    "#finding all parent chains of all matched words\n",
    "\n",
    "all_chains = []\n",
    "for i in range(len(matches_sorted)): \n",
    "    #print(matches_sorted[i])\n",
    "    parent = []\n",
    "    parent_chain = [matches_sorted[i]]\n",
    "    findParent(matches_sorted[i], parent_chain)\n",
    "    all_chains.append(parent_chain)\n",
    "    #print(parent_chain) \n",
    "\n",
    "if print_all_chains:\n",
    "    print(\"\\nALL CHAINS: \")\n",
    "    print (\"Total number of chains: \" + str(len(all_chains)))\n",
    "    for i in all_chains:\n",
    "        print(i)\n",
    "   \n",
    "\n",
    "\n",
    " ##########################################  CREATE TREE WITH PATHS   ###########################################\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self,data=''):\n",
    "        self.visited = False\n",
    "        self.data = data\n",
    "        self.child = []\n",
    "        \n",
    "    def createNode(self, data):\n",
    "        return Node(data)\n",
    "    \n",
    "    def createChildren(self,info):\n",
    "        for i in range(len(info)):\n",
    "            n = self.createNode(info[i])\n",
    "            self.child.append(n)\n",
    "    \n",
    "    def add_children(self, count): \n",
    "        for i in range(len(self.child)):\n",
    "            kids = set([])\n",
    "            for j in range(len(all_chains)):\n",
    "                if (count >= len(all_chains[j])):\n",
    "                    continue\n",
    "                if (count < len(all_chains[j])):    \n",
    "                    if (all_chains[j][count - 1] == self.child[i].data):\n",
    "                        kids.add(all_chains[j][count])\n",
    "            if (len(list(kids)) == 0):\n",
    "                return\n",
    "            self.child[i].createChildren(list(kids))\n",
    "            self.child[i].add_children(count+1)\n",
    "          \n",
    "    def traverse(self,local_path):\n",
    "        path = []\n",
    "        if (self.data is not ''):\n",
    "            local_path.append(self.data)\n",
    "        if len(self.child) != 0:\n",
    "            for n in self.child:\n",
    "                path.extend(n.traverse(local_path[:]))\n",
    "        else:\n",
    "            path.append(local_path)\n",
    "        return path\n",
    "\n",
    "\n",
    "'''TRAVERSE THROUGH THE TREE,\n",
    "    PRINT OUT A SIMPLIFIED LIST OF PARENT CHAIN PATHS\n",
    "    ie, A -> B -> C -> D\n",
    "    [A, B, C] [A, B] -> ONLY [A, B, C] IS PRINTED'''\n",
    "\n",
    "root = Node()\n",
    "\n",
    "#initializing first layer of tree\n",
    "first_level = set([])\n",
    "for i in range(len(all_chains)):\n",
    "    first_level.add(all_chains[i][0])\n",
    "root.createChildren(list(first_level))\n",
    "\n",
    "#create the rest of the tree\n",
    "root.add_children(1)\n",
    "\n",
    "#traverse through the paths of the tree\n",
    "path = root.traverse([])\n",
    "if print_simplified_chains:\n",
    "    print(\"\\nSIMPLIFIED CHAINS: \")\n",
    "    print (\"Total number of chains: \"+str(len(path)))\n",
    "    for i in path:\n",
    "        print(i[:number_of_levels])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
